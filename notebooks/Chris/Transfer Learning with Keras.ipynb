{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "In this notebook, I am attempting to do tranfer learning with Keras, Tensorflow. This is based on the tutorial at [keras.org](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n",
    "\n",
    "I modified the script located in a [gist](https://gist.githubusercontent.com/fchollet/f35fbc80e066a49d65f1688a7e99f069/raw/04f05ef9d573acb503476d07123097ba99181f3c/classifier_from_little_data_script_2.py). \n",
    "\n",
    "#### Load the required modules and set path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications import vgg16\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import optimizers\n",
    "from os.path import join, split\n",
    "from os import getcwd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import convert_kernel\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# path to the model weights file\n",
    "base = join(split(split(getcwd())[0])[0],'data_local','dogs_cats')\n",
    "weights_path = join(split(base)[0],'vgg16_weights.h5')\n",
    "top_model_weights_path = join(base, 'bottleneck_fc_model.h5')\n",
    "\n",
    "# dimensions of the imates\n",
    "img_width , img_height = 150, 150\n",
    "\n",
    "train_data_dir = join(base,'train')\n",
    "validation_data_dir = join(base,'validation')\n",
    "nb_train_samples = 2000\n",
    "nb_validation_sampeles = 800\n",
    "nb_epoch = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_bottleneck_feature(train_data_dir, validation_data_dir, save_directory,\n",
    "                           img_width =150, img_height=150,nb_train_samples=62*32,\n",
    "                            nb_validation_samples=800):\n",
    "    \"\"\"Predicts with the convolutional layers of VGG16 and saves the output\n",
    "    Args: \n",
    "        train_data_dir(str): path to training data\n",
    "        validation__data_dir(str): path to validation data\n",
    "        save_directory(str): path to save the files\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "\n",
    "    # load the vgg16 covolutional layers\n",
    "    model = vgg16.VGG16(include_top=False,input_shape=( img_width, img_height,3))\n",
    "    \n",
    "    # set up a data generator for the training data\n",
    "    datagen=ImageDataGenerator(1./255)\n",
    "    generator = datagen.flow_from_directory(\n",
    "            train_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "    \n",
    "    # predict on the training data\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples)\n",
    "    file_name = join(save_directory, 'bottleneck_features_train.npy')\n",
    "    np.save(open(file_name, 'w'), bottleneck_features_train)\n",
    "    \n",
    "    # set up a generator for the validation_data\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode = None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    # predict on the validation data\n",
    "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples)\n",
    "    file_name = join(save_directory, 'bottleneck_features_validation.npy')\n",
    "    np.save(open(file_name, 'w'), bottleneck_features_validation)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_feature(train_data_dir=train_data_dir, validation_data_dir=validation_data_dir,\n",
    "                       save_directory=base, nb_train_samples=62*32, nb_validation_samples=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_top_model(save_directory,top_model_weights_path,\n",
    "                    nb_train_samples=32*62, nb_validation_samples=800,\n",
    "                   nb_epochs=50 ):\n",
    "    \"\"\" Trains the top model weights\"\"\"\n",
    "    file_name = join(save_directory, 'bottleneck_features_train.npy')\n",
    "    train_data = np.load(open(file_name))\n",
    "    train_labels = np.array([0] * (nb_train_samples / 2) + [1] * (nb_train_samples / 2))\n",
    "    \n",
    "    \n",
    "    file_name = join(save_directory, 'bottleneck_features_validation.npy')\n",
    "    validation_data = np.load(open(file_name))\n",
    "    validation_labels = np.array([0] * (nb_validation_samples / 2) + [1] * (nb_validation_samples / 2))\n",
    "\n",
    "    # set up the \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(validation_data, validation_labels), \n",
    "             verbose = 1)\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    return model\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1984 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "1984/1984 [==============================] - 1s - loss: 3.0749 - acc: 0.7984 - val_loss: 3.7050 - val_acc: 0.7612\n",
      "Epoch 2/50\n",
      "1984/1984 [==============================] - 1s - loss: 2.0987 - acc: 0.8599 - val_loss: 1.1869 - val_acc: 0.9187\n",
      "Epoch 3/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.5605 - acc: 0.8962 - val_loss: 1.1901 - val_acc: 0.9200\n",
      "Epoch 4/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.2378 - acc: 0.9204 - val_loss: 1.0665 - val_acc: 0.9287\n",
      "Epoch 5/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.2872 - acc: 0.9153 - val_loss: 0.9512 - val_acc: 0.9350\n",
      "Epoch 6/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.2671 - acc: 0.9183 - val_loss: 1.0268 - val_acc: 0.9325\n",
      "Epoch 7/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.1348 - acc: 0.9264 - val_loss: 1.5012 - val_acc: 0.9025\n",
      "Epoch 8/50\n",
      "1984/1984 [==============================] - 2s - loss: 1.0541 - acc: 0.9304 - val_loss: 1.1032 - val_acc: 0.9275\n",
      "Epoch 9/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.9648 - acc: 0.9380 - val_loss: 1.0507 - val_acc: 0.9313\n",
      "Epoch 10/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.0847 - acc: 0.9284 - val_loss: 1.4841 - val_acc: 0.9025\n",
      "Epoch 11/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.1704 - acc: 0.9254 - val_loss: 1.6937 - val_acc: 0.8900\n",
      "Epoch 12/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.0426 - acc: 0.9330 - val_loss: 0.9156 - val_acc: 0.9400\n",
      "Epoch 13/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.9102 - acc: 0.9420 - val_loss: 0.9218 - val_acc: 0.9425\n",
      "Epoch 14/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.9567 - acc: 0.9400 - val_loss: 1.7845 - val_acc: 0.8850\n",
      "Epoch 15/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.0127 - acc: 0.9345 - val_loss: 1.2003 - val_acc: 0.9250\n",
      "Epoch 16/50\n",
      "1984/1984 [==============================] - 1s - loss: 1.0085 - acc: 0.9360 - val_loss: 1.1318 - val_acc: 0.9275\n",
      "Epoch 17/50\n",
      "1984/1984 [==============================] - 2s - loss: 1.1464 - acc: 0.9264 - val_loss: 1.3672 - val_acc: 0.9137\n",
      "Epoch 18/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.9330 - acc: 0.9410 - val_loss: 1.2379 - val_acc: 0.9200\n",
      "Epoch 19/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.9994 - acc: 0.9370 - val_loss: 1.0541 - val_acc: 0.9313\n",
      "Epoch 20/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.9057 - acc: 0.9410 - val_loss: 0.9866 - val_acc: 0.9350\n",
      "Epoch 21/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7739 - acc: 0.9501 - val_loss: 0.9084 - val_acc: 0.9425\n",
      "Epoch 22/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.8915 - acc: 0.9430 - val_loss: 0.8898 - val_acc: 0.9425\n",
      "Epoch 23/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7927 - acc: 0.9496 - val_loss: 1.0774 - val_acc: 0.9287\n",
      "Epoch 24/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7318 - acc: 0.9511 - val_loss: 1.0847 - val_acc: 0.9300\n",
      "Epoch 25/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.8589 - acc: 0.9446 - val_loss: 0.8422 - val_acc: 0.9475\n",
      "Epoch 26/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.9004 - acc: 0.9425 - val_loss: 1.4131 - val_acc: 0.9113\n",
      "Epoch 27/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.7664 - acc: 0.9506 - val_loss: 1.0465 - val_acc: 0.9325\n",
      "Epoch 28/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.8926 - acc: 0.9430 - val_loss: 1.0249 - val_acc: 0.9325\n",
      "Epoch 29/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.8704 - acc: 0.9446 - val_loss: 0.9282 - val_acc: 0.9413\n",
      "Epoch 30/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7429 - acc: 0.9526 - val_loss: 1.4122 - val_acc: 0.9113\n",
      "Epoch 31/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.6381 - acc: 0.9582 - val_loss: 1.0646 - val_acc: 0.9300\n",
      "Epoch 32/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.6053 - acc: 0.9617 - val_loss: 1.0238 - val_acc: 0.9337\n",
      "Epoch 33/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.6606 - acc: 0.9582 - val_loss: 0.9559 - val_acc: 0.9387\n",
      "Epoch 34/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.6724 - acc: 0.9577 - val_loss: 0.9822 - val_acc: 0.9350\n",
      "Epoch 35/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7435 - acc: 0.9511 - val_loss: 0.8340 - val_acc: 0.9450\n",
      "Epoch 36/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.5737 - acc: 0.9637 - val_loss: 0.8320 - val_acc: 0.9463\n",
      "Epoch 37/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.6219 - acc: 0.9602 - val_loss: 0.8371 - val_acc: 0.9463\n",
      "Epoch 38/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7082 - acc: 0.9526 - val_loss: 1.2816 - val_acc: 0.9175\n",
      "Epoch 39/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.7219 - acc: 0.9546 - val_loss: 1.0164 - val_acc: 0.9363\n",
      "Epoch 40/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.8041 - acc: 0.9496 - val_loss: 0.8248 - val_acc: 0.9450\n",
      "Epoch 41/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.5695 - acc: 0.9642 - val_loss: 0.8933 - val_acc: 0.9437\n",
      "Epoch 42/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.5040 - acc: 0.9667 - val_loss: 0.8261 - val_acc: 0.9463\n",
      "Epoch 43/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.5586 - acc: 0.9652 - val_loss: 0.7863 - val_acc: 0.9500\n",
      "Epoch 44/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.5608 - acc: 0.9637 - val_loss: 0.9334 - val_acc: 0.9400\n",
      "Epoch 45/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.5020 - acc: 0.9688 - val_loss: 0.7807 - val_acc: 0.9500\n",
      "Epoch 46/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.6077 - acc: 0.9612 - val_loss: 0.8512 - val_acc: 0.9450\n",
      "Epoch 47/50\n",
      "1984/1984 [==============================] - 2s - loss: 0.5329 - acc: 0.9667 - val_loss: 0.9180 - val_acc: 0.9413\n",
      "Epoch 48/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.4846 - acc: 0.9688 - val_loss: 1.0472 - val_acc: 0.9313\n",
      "Epoch 49/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.4743 - acc: 0.9703 - val_loss: 0.9783 - val_acc: 0.9363\n",
      "Epoch 50/50\n",
      "1984/1984 [==============================] - 1s - loss: 0.4718 - acc: 0.9698 - val_loss: 0.9888 - val_acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "top_model = train_top_model(save_directory = base,top_model_weights_path=top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights_path():\n",
    "    \"\"\"gets the local path to the weights\"\"\"\n",
    "    TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    return get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                        TF_WEIGHTS_PATH_NO_TOP,\n",
    "                                        cache_subdir='models')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_alternate_VGG16_model(top_model=None, img_width = 150, img_height = 150,\n",
    "                               freeze_layers = 25,loss=None, optimizer=None, metrics=None):\n",
    "    \"\"\"Sets up an alternate vgg16 model with diffent top\"\"\"\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height,3)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # Load the weights\n",
    "    model.load_weights(get_weights_path())\n",
    "    \n",
    "    if top_model is None:\n",
    "        # build a classifier model to put on top of the convolutional model\n",
    "        top_model = Sequential()\n",
    "        top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "        top_model.add(Dense(256, activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        top_model_weights_path = base = join(split(split(getcwd())[0])[0],'data_local','dogs_cats',\n",
    "                                       'bottleneck_fc_model.h5')\n",
    "        \n",
    "        top_model.load_weights(top_model_weights_path)\n",
    "        \n",
    "    \n",
    "    model.add(top_model)\n",
    "    \n",
    "    # freeze the first 25 layers\n",
    "    # set the first 25 layers (up to the last conv block)\n",
    "    for layer in model.layers[:freeze_layers]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # compile the model to be optimzied\n",
    "    if loss is None: \n",
    "        loss = 'binary_crossentropy'\n",
    "    if optimizer is None: \n",
    "        optimizer = optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "    if metrics is None:\n",
    "        metrics = ['accuracy']\n",
    "\n",
    "    model.compile(loss =loss, \n",
    "                  optimizer = optimizer,\n",
    "                  metrics = metrics)\n",
    "                  \n",
    "    \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = setup_alternate_VGG16_model(top_model=None, img_width = 150, img_height = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the training of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1984/2000 [============================>.] - ETA: 4s - loss: 1.2838 - acc: 0.7908 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crivera5/.virtual_envs/Kaggle/lib/python2.7/site-packages/keras/engine/training.py:1527: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002/2000 [==============================] - 778s - loss: 1.2812 - acc: 0.7917 - val_loss: 0.3249 - val_acc: 0.9075\n",
      "Epoch 2/50\n",
      "2002/2000 [==============================] - 763s - loss: 0.4976 - acc: 0.8616 - val_loss: 0.2861 - val_acc: 0.9175\n",
      "Epoch 3/50\n",
      "2002/2000 [==============================] - 745s - loss: 0.3825 - acc: 0.8856 - val_loss: 0.2765 - val_acc: 0.9175\n",
      "Epoch 4/50\n",
      "2002/2000 [==============================] - 741s - loss: 0.2906 - acc: 0.9001 - val_loss: 0.2620 - val_acc: 0.9175\n",
      "Epoch 5/50\n",
      "2002/2000 [==============================] - 738s - loss: 0.2344 - acc: 0.9201 - val_loss: 0.2255 - val_acc: 0.9225\n",
      "Epoch 6/50\n",
      "2002/2000 [==============================] - 731s - loss: 0.2001 - acc: 0.9206 - val_loss: 0.2141 - val_acc: 0.9237\n",
      "Epoch 7/50\n",
      "2002/2000 [==============================] - 733s - loss: 0.1718 - acc: 0.9356 - val_loss: 0.2127 - val_acc: 0.9213\n",
      "Epoch 8/50\n",
      "2002/2000 [==============================] - 733s - loss: 0.1474 - acc: 0.9436 - val_loss: 0.2064 - val_acc: 0.9225\n",
      "Epoch 9/50\n",
      "2002/2000 [==============================] - 732s - loss: 0.1358 - acc: 0.9486 - val_loss: 0.2088 - val_acc: 0.9225\n",
      "Epoch 10/50\n",
      "2002/2000 [==============================] - 731s - loss: 0.1292 - acc: 0.9496 - val_loss: 0.2174 - val_acc: 0.9325\n",
      "Epoch 11/50\n",
      "2002/2000 [==============================] - 729s - loss: 0.1383 - acc: 0.9461 - val_loss: 0.2144 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "2002/2000 [==============================] - 728s - loss: 0.1045 - acc: 0.9605 - val_loss: 0.2009 - val_acc: 0.9300\n",
      "Epoch 13/50\n",
      "2002/2000 [==============================] - 728s - loss: 0.0974 - acc: 0.9610 - val_loss: 0.1963 - val_acc: 0.9288\n",
      "Epoch 14/50\n",
      "2002/2000 [==============================] - 729s - loss: 0.0787 - acc: 0.9700 - val_loss: 0.2015 - val_acc: 0.9312\n",
      "Epoch 15/50\n",
      "2002/2000 [==============================] - 725s - loss: 0.0917 - acc: 0.9630 - val_loss: 0.1946 - val_acc: 0.9312\n",
      "Epoch 16/50\n",
      "2002/2000 [==============================] - 734s - loss: 0.0892 - acc: 0.9710 - val_loss: 0.2095 - val_acc: 0.9300\n",
      "Epoch 17/50\n",
      "2002/2000 [==============================] - 726s - loss: 0.0724 - acc: 0.9720 - val_loss: 0.2187 - val_acc: 0.9275\n",
      "Epoch 18/50\n",
      "2002/2000 [==============================] - 730s - loss: 0.0757 - acc: 0.9700 - val_loss: 0.2085 - val_acc: 0.9325\n",
      "Epoch 19/50\n",
      "2002/2000 [==============================] - 727s - loss: 0.0669 - acc: 0.9785 - val_loss: 0.2409 - val_acc: 0.9237\n",
      "Epoch 20/50\n",
      "2002/2000 [==============================] - 735s - loss: 0.0643 - acc: 0.9795 - val_loss: 0.2539 - val_acc: 0.9200\n",
      "Epoch 21/50\n",
      "2002/2000 [==============================] - 738s - loss: 0.0664 - acc: 0.9740 - val_loss: 0.2124 - val_acc: 0.9237\n",
      "Epoch 22/50\n",
      "2002/2000 [==============================] - 734s - loss: 0.0468 - acc: 0.9845 - val_loss: 0.2619 - val_acc: 0.9150\n",
      "Epoch 23/50\n",
      "2002/2000 [==============================] - 733s - loss: 0.0644 - acc: 0.9740 - val_loss: 0.1976 - val_acc: 0.9362\n",
      "Epoch 24/50\n",
      "2002/2000 [==============================] - 727s - loss: 0.0555 - acc: 0.9800 - val_loss: 0.2066 - val_acc: 0.9237\n",
      "Epoch 25/50\n",
      "2002/2000 [==============================] - 727s - loss: 0.0348 - acc: 0.9910 - val_loss: 0.2148 - val_acc: 0.9312\n",
      "Epoch 26/50\n",
      "2002/2000 [==============================] - 731s - loss: 0.0432 - acc: 0.9870 - val_loss: 0.2095 - val_acc: 0.9350\n",
      "Epoch 27/50\n",
      "2002/2000 [==============================] - 727s - loss: 0.0457 - acc: 0.9835 - val_loss: 0.2520 - val_acc: 0.9275\n",
      "Epoch 28/50\n",
      "2002/2000 [==============================] - 731s - loss: 0.0486 - acc: 0.9830 - val_loss: 0.1970 - val_acc: 0.9375\n",
      "Epoch 29/50\n",
      "2002/2000 [==============================] - 728s - loss: 0.0423 - acc: 0.9850 - val_loss: 0.2382 - val_acc: 0.9237\n",
      "Epoch 30/50\n",
      "2002/2000 [==============================] - 726s - loss: 0.0442 - acc: 0.9855 - val_loss: 0.2360 - val_acc: 0.9263\n",
      "Epoch 31/50\n",
      "2002/2000 [==============================] - 722s - loss: 0.0261 - acc: 0.9920 - val_loss: 0.2379 - val_acc: 0.9312\n",
      "Epoch 32/50\n",
      "2002/2000 [==============================] - 728s - loss: 0.0300 - acc: 0.9905 - val_loss: 0.2326 - val_acc: 0.9362\n",
      "Epoch 33/50\n",
      "2002/2000 [==============================] - 726s - loss: 0.0282 - acc: 0.9885 - val_loss: 0.2100 - val_acc: 0.9338\n",
      "Epoch 34/50\n",
      "2002/2000 [==============================] - 724s - loss: 0.0283 - acc: 0.9880 - val_loss: 0.2388 - val_acc: 0.9338\n",
      "Epoch 35/50\n",
      "2002/2000 [==============================] - 729s - loss: 0.0266 - acc: 0.9920 - val_loss: 0.2632 - val_acc: 0.9213\n",
      "Epoch 36/50\n",
      "2002/2000 [==============================] - 724s - loss: 0.0287 - acc: 0.9895 - val_loss: 0.2335 - val_acc: 0.9325\n",
      "Epoch 37/50\n",
      "2002/2000 [==============================] - 722s - loss: 0.0283 - acc: 0.9870 - val_loss: 0.2309 - val_acc: 0.9300\n",
      "Epoch 38/50\n",
      "2002/2000 [==============================] - 726s - loss: 0.0251 - acc: 0.9905 - val_loss: 0.2362 - val_acc: 0.9325\n",
      "Epoch 39/50\n",
      "2002/2000 [==============================] - 724s - loss: 0.0243 - acc: 0.9915 - val_loss: 0.2172 - val_acc: 0.9350\n",
      "Epoch 40/50\n",
      "2002/2000 [==============================] - 728s - loss: 0.0248 - acc: 0.9935 - val_loss: 0.2333 - val_acc: 0.9375\n",
      "Epoch 41/50\n",
      "2002/2000 [==============================] - 723s - loss: 0.0310 - acc: 0.9880 - val_loss: 0.2277 - val_acc: 0.9350\n",
      "Epoch 42/50\n",
      "2002/2000 [==============================] - 723s - loss: 0.0280 - acc: 0.9935 - val_loss: 0.2191 - val_acc: 0.9375\n",
      "Epoch 43/50\n",
      "2002/2000 [==============================] - 725s - loss: 0.0176 - acc: 0.9940 - val_loss: 0.2374 - val_acc: 0.9375\n",
      "Epoch 44/50\n",
      "2002/2000 [==============================] - 723s - loss: 0.0177 - acc: 0.9950 - val_loss: 0.2273 - val_acc: 0.9362\n",
      "Epoch 45/50\n",
      "2002/2000 [==============================] - 723s - loss: 0.0187 - acc: 0.9950 - val_loss: 0.2228 - val_acc: 0.9412\n",
      "Epoch 46/50\n",
      "2002/2000 [==============================] - 720s - loss: 0.0155 - acc: 0.9975 - val_loss: 0.2493 - val_acc: 0.9288\n",
      "Epoch 47/50\n",
      "2002/2000 [==============================] - 722s - loss: 0.0165 - acc: 0.9950 - val_loss: 0.2457 - val_acc: 0.9350\n",
      "Epoch 48/50\n",
      "2002/2000 [==============================] - 722s - loss: 0.0203 - acc: 0.9945 - val_loss: 0.2329 - val_acc: 0.9350\n",
      "Epoch 49/50\n",
      "2002/2000 [==============================] - 723s - loss: 0.0134 - acc: 0.9960 - val_loss: 0.2364 - val_acc: 0.9350\n",
      "Epoch 50/50\n",
      "2002/2000 [==============================] - 723s - loss: 0.0168 - acc: 0.9950 - val_loss: 0.2298 - val_acc: 0.9250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15c3cffd0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "This general method worked for doing the training on the dog cat example. We can use it to base our initial training of the other models. \n",
    "\n",
    "The one issue is that we will want to use the a different top model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.models.Sequential"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
